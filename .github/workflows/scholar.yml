- name: Fetch Google Scholar Data
  env:
    SCHOLAR_ID: ${{ secrets.SCHOLAR_ID }}
  run: |
    python -c "
    from scholarly import scholarly, ProxyGenerator
    import yaml
    import time

    retries = 5
    pg = ProxyGenerator()
    pg.FreeProxies()
    scholarly.use_proxy(pg)

    for i in range(retries):
        try:
            author = scholarly.search_author_id('$SCHOLAR_ID')
            scholarly.fill(author, sections=['publications'])
            break
        except Exception as e:
            print(f'Retry {i+1}/{retries}: {e}')
            time.sleep(2 ** i)
    else:
        raise Exception('Failed to fetch data after retries')

    pubs = author['publications'][:25]
    print(f'Found {len(pubs)} publications')
    data = {'publications': []}
    for pub in pubs:
        bib = pub.get('bib', {})
        data['publications'].append({
            'title': bib.get('title', ''),
            'authors': bib.get('author', ''),
            'year': bib.get('year', ''),
            'venue': bib.get('journal', bib.get('conference', '')),
            'url': pub.get('pub_url', ''),
            'doi': next((link.split('doi.org/')[1] for link in bib.get('url', '').split() if 'doi.org/' in link), '')
        })
    with open('_data/scholar.yml', 'w') as f:
        yaml.dump(data, f)
    "
